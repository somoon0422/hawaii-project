# import streamlit as st
# import pandas as pd
# import matplotlib.pyplot as plt
# from wordcloud import WordCloud
# from collections import Counter
# import seaborn as sns

# # CSV íŒŒì¼ ì½ê¸°
# data = pd.read_csv('Total_News.csv')
# df = pd.DataFrame(data)

# # ì‚¬ì´ë“œë°”ì— í˜ì´ì§€ ì„ íƒ ìƒì ì¶”ê°€
# page = st.sidebar.selectbox(
#     "í˜ì´ì§€ ì„ íƒ",
#     ["ë‰´ìŠ¤ ë°ì´í„°", "Data visualization"]
# )

# # ë‰´ìŠ¤ ë°ì´í„° í˜ì´ì§€
# if page == "ë‰´ìŠ¤ ë°ì´í„°":
#     # ëŒ€ì‹œë³´ë“œ ì œëª© ë° ì„¤ëª…
#     st.title("ğŸï¸ í•˜ì™€ì´ í”„ë¡œì íŠ¸ - ë‰´ìŠ¤ í¬ë¡¤ë§")

#     st.write(
#         "ì•ˆë…•í•˜ì„¸ìš”ğŸ‘‹ "
#         "í•˜ì™€ì´ í”„ë¡œì íŠ¸ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤."
#     )

#     st.write(
#         "ì•„ë˜ëŠ” í¬ë¡¤ë§ì— ì‚¬ìš©ëœ data ì…ë‹ˆë‹¤." 
#         "í¬ë¡¤ë§ì´ ì§„í–‰ë˜ë©´ ì—¬ê¸°ì„œ í™•ì¸ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
#         "ì´ì œ ì›í•˜ëŠ” ë¶„ì„ê³¼ ì‹œê°í™”ë¥¼ í™•ì¸ í•˜ì„¸ìš”."
#     )

#     # ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
#     st.write(df)

# # Data visualization í˜ì´ì§€
# elif page == "Data visualization":
#     st.title("ğŸ“Š Data visualization")

#     st.write(
#         "ì—¬ê¸°ì—ì„œ ë‹¤ì–‘í•œ ë°ì´í„° ì‹œê°í™”ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
#     )

#     # ì˜ˆì‹œ: ì›Œë“œ í´ë¼ìš°ë“œ ì‹œê°í™”
#     st.subheader("Word Cloud of Text Data")
#     text_column = st.selectbox("í…ìŠ¤íŠ¸ ë°ì´í„° ì»¬ëŸ¼ ì„ íƒ", df.columns)
#     text_data = df[text_column].dropna().astype(str).values
#     text = " ".join(text_data)
    
#     wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    
#     plt.figure(figsize=(10, 5))
#     plt.imshow(wordcloud, interpolation='bilinear')
#     plt.axis('off')
#     st.pyplot(plt)

#     # ì˜ˆì‹œ: íŠ¹ì • ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ì‹œê°í™”
#     st.subheader("Top 10 Frequent Words")
#     words = text.split()
#     word_counts = Counter(words)
#     common_words = word_counts.most_common(10)
    
#     word_df = pd.DataFrame(common_words, columns=['word', 'count'])
    
#     plt.figure(figsize=(10, 5))
#     sns.barplot(x='count', y='word', data=word_df, palette='viridis')
#     st.pyplot(plt)


import streamlit as st
import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from textblob import TextBlob
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Pandas ì„¤ì • ë³€ê²½: ìµœëŒ€ ì—´ ìˆ˜ì™€ ìµœëŒ€ í–‰ ìˆ˜ ì„¤ì •
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# í˜ì´ì§€ ì œëª©
st.title("ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

# ë°ì´í„° ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (CSV í˜•ì‹)")

if uploaded_file is not None:
    # ë°ì´í„° ì½ê¸°
    df = pd.read_csv(uploaded_file)
    st.write("ì—…ë¡œë“œëœ ë°ì´í„°:")
    
    # ë°ì´í„° í”„ë ˆì„ì„ ìŠ¤í¬ë¡¤ ê°€ëŠ¥í•˜ê²Œ í‘œì‹œ
    st.dataframe(df, height=600, width=800)

    # ë°ì´í„° í”„ë ˆì„ì˜ ì—´ ì´ë¦„ ì¶œë ¥
    st.write("ë°ì´í„° í”„ë ˆì„ì˜ ì—´ ì´ë¦„:")
    st.write(df.columns)

    # 'Country', 'Article Title', 'Article Body' ì—´ì´ ìˆëŠ”ì§€ í™•ì¸
    if 'Country' in df.columns and 'Article Title' in df.columns and 'Article Body' in df.columns:
        # êµ­ê°€ë³„ë¡œ ë°ì´í„° ë¶„ë¥˜
        countries = df['Country'].unique()
        selected_country = st.selectbox("êµ­ê°€ë¥¼ ì„ íƒí•˜ì„¸ìš”", countries)

        # ì„ íƒëœ êµ­ê°€ì˜ ë°ì´í„° í•„í„°ë§
        country_data = df[df['Country'] == selected_country]

        # ë°ì´í„° ìš”ì•½
        st.header(f"{selected_country} ë°ì´í„° ìš”ì•½")
        num_articles = len(country_data)
        st.write(f"ì´ ê¸°ì‚¬ ìˆ˜: {num_articles}")

        # ê¸°ì‚¬ ì œëª©ê³¼ ë³¸ë¬¸ ë‚´ìš©ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        country_data['text'] = country_data['Article Title'] + " " + country_data['Article Body']

        # ë¹ˆë„ ë¶„ì„
        st.header("ë¹ˆë„ ë¶„ì„")
        vectorizer = CountVectorizer(stop_words='english')
        word_count = vectorizer.fit_transform(country_data['text'])
        word_count_sum = word_count.sum(axis=0)
        words_freq = [(word, word_count_sum[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
        st.write(words_freq[:10])

        # ê°ì • ë¶„ì„
        st.header("ê°ì • ë¶„ì„")
        country_data['polarity'] = country_data['text'].apply(lambda x: TextBlob(x).sentiment.polarity)
        st.write(country_data[['Article Title', 'polarity']].head())
        st.bar_chart(country_data['polarity'])

        # ì›Œë“œ í´ë¼ìš°ë“œ
        st.header("ì›Œë“œ í´ë¼ìš°ë“œ")
        wordcloud = WordCloud(width=800, height=400, max_words=100).generate(' '.join(country_data['text']))
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        st.pyplot(plt)

        # í† í”½ ëª¨ë¸ë§
        st.header("í† í”½ ëª¨ë¸ë§")
        lda = LatentDirichletAllocation(n_components=5, random_state=0)
        lda.fit(word_count)
        terms = vectorizer.get_feature_names_out()
        for idx, topic in enumerate(lda.components_):
            st.write(f"í† í”½ {idx+1}:")
            st.write(" ".join([terms[i] for i in topic.argsort()[:-11:-1]]))
    else:
        st.write("'Country', 'Article Title', 'Article Body' ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° íŒŒì¼ì— ì´ ì—´ë“¤ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
else:
    st.write("íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
